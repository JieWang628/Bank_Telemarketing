---
title: "Final_535"
author: "Jie Wang"
date: "December 3, 2017"
output: pdf_document
---

##Bank Direct Marketing Campaign

dataset
```{r}
rm(list=ls())
bank_full <- read.csv(file.choose(),header=TRUE,sep = ";")
anyNA(bank_full)
summary(bank_full)
```

Split train set and test set
```{r}
library(ggplot2)
library(lattice)
library(caret)
train.index <- createDataPartition(bank_full$y, p = 0.6, list = FALSE)
train <- bank_full[train.index,]
test <- bank_full[-train.index,]
dim(train)
dim(test)
ytest <- test$y
test$y <- NULL
dim(test)
```


Some data visualization work here by ggplot2

Feature selection:
Firstly, introduce the R package `Fselector`. Then, we use it to deal with our dataset.
Feature selection (3 ways)
```{r}
library(mlbench)
library(FSelector)
weights3 <- random.forest.importance(y~.,train,importance.type = 1)
print(weights3)
```
use the cutoff function to obtain the attributes of the top six weights:
```{r}
subset <- cutoff.k(weights3, 5)
f <- as.simple.formula(subset, "y")
print(f)
```


```{r}
library(mlbench)
library(FSelector)
weights1 <- chi.squared(y~.,train)
subset <- cutoff.k(weights1, 5)
subset
weights2 <- information.gain(y~.,train)
subset <- cutoff.k(weights2, 5)
f <- as.simple.formula(subset, "y")

```


#glm
```{r}
fit_glm<-glm(y~.,family=binomial(link='logit'),data=train)
summary(fit_glm)
```

```{r}
library(ROCR)
p <- predict(fit_glm, newdata=test, type="response")
pr <- prediction(p, ytest)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf,col="green",lwd=2,main="ROC Curve for Logistic Regression")
abline(a=0,b=1,lwd=2,lty=2,col="gray")
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
```
Decision Tree
```{r}
library(rpart)
# grow tree
tree_fit <- rpart(y~., data=train, method = "class", minsplit=1000, maxdepth=30, cp=0.001)
printcp(tree_fit)
plotcp(tree_fit)
summary(tree_fit)
```

```{r}
# plot tree 
plot(tree_fit, uniform=TRUE, main="Classification Tree")
text(tree_fit, use.n=TRUE, all=TRUE, cex=.8)
# create attractive postscript plot of tree 
post(tree_fit, title = "Classification Tree ")
```
```{r}
library(ROCR)
pp <- predict(tree_fit, newdata=test)
pr <- prediction(pp[,2], ytest)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf, text.adj = c(-0.2,1.7))
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
```

```{r}
library(tree)
fit_tree <- tree(f, data=train)
summary(fit_tree)
plot(fit_tree)
text(fit_tree, all=T)
cv.tree(fit_tree)
```
```{r}
tree2 <- prune.misclass(fit_tree, best=9)
summary(tree2)
```
```{r}
library(ROCR)
pp <- predict(tree2, newdata=test)
pr <- prediction(pp[,2], ytest)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf,col="blue",lwd=2,main="ROC Curve for Random Forest")
abline(a=0,b=1,lwd=2,lty=2,col="gray")
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
```

Random forest
```{r}
library(randomForest)
rf_fit <- randomForest(y~., data=train, importance=TRUE, ntree=500)
print(rf_fit)
## Look at variable importance:
round(importance(rf_fit), 2)
```
```{r}
plot(rf_fit,log="y", main="Error Plot")
legend("right", colnames(rf_fit$err.rate),col=1:3,cex=1.1,fill=1:3)
varImpPlot(rf_fit, main="Variable Importance", type=1)
```

```{r}
library(ROCR)
pp <- predict(rf_fit, newdata=test,type = "prob")
pr <- prediction(pp[,2], ytest)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf,col="blue",lwd=2,main="ROC Curve for Random Forest")
abline(a=0,b=1,lwd=2,lty=2,col="gray")
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
```

MCMC
```{r}
#MCMC
set.seed(100)
logpost <- function(beta){
  p <- (1+exp(-X%*%beta))^(-1)
  lik <- p^y*(1-p)^(1-y)
  prior <- dmvnorm(t(beta),rep(0,9),diag(rep(1,9)))
  logprob <- log(prior)+sum(log(lik))
  return(logprob)
}
 
I=51000
ptm <- proc.time()
beta1 = matrix(rep(NA,17*I),nrow=17)
beta1[,1] <- t(rmvnorm(1,rep(0,17),diag(c(1,0.01,1,1,1,0.01,0.01,0.1,1,1,0.01,1,1,1,0.01,0.01,0.1))))    # starting location for random walk
accepted1 = c(1)
for(t in 2:I)    {
  beta.prop <-t(rmvnorm(1,beta1[,t-1],diag(c(1,0.01,1,1,1,0.01,0.01,0.1,1,1,0.01,1,1,1,0.01,0.01,0.1)))) # implementation assumes a random walk. 
  # discard this input for a fixed proposal distribution
  
  # We work with the log-likelihoods for numeric stability.
  logR = logpost(beta.prop)-logpost(beta1[,t-1])
  
  R = exp(logR)
  
  u <- runif(1)        ## uniform variable to determine acceptance
  if(u < R){           ## accept the new value
    beta1[,t] = beta.prop
    accepted1 = c(accepted1,1)
  }    
  else{
    beta1[,t] = beta1[,t-1]     ## reject the new value
    accepted1 = c(accepted1,0)
  }    
}
beta2 = matrix(rep(NA,9*I),nrow=9)
beta2[,1] <- t(rmvnorm(1,rep(0,9),diag(c(1,0.01,1,1,1,0.01,0.01,0.1,1))))    # starting location for random walk
accepted2 = c(1)
for(t in 2:I)    {
  beta.prop <-t(rmvnorm(1,beta2[,t-1],diag(c(1,0.01,1,1,1,0.01,0.01,0.1,1)))) # implementation assumes a random walk. 
  # discard this input for a fixed proposal distribution
  
  # We work with the log-likelihoods for numeric stability.
  logR = logpost(beta.prop)-logpost(beta2[,t-1])
  
  R = exp(logR)
  
  u <- runif(1)        ## uniform variable to determine acceptance
  if(u < R){           ## accept the new value
    beta2[,t] = beta.prop
    accepted2 = c(accepted2,1)
  }    
  else{
    beta2[,t] = beta2[,t-1]     ## reject the new value
    accepted2 = c(accepted2,0)
  }    
}
beta3 = matrix(rep(NA,9*I),nrow=9)
beta3[,1] <- t(rmvnorm(1,rep(0,9),diag(c(1,0.01,1,1,1,0.01,0.01,0.1,1))))    # starting location for random walk
accepted3 = c(1)
for(t in 2:I)    {
  beta.prop <-t(rmvnorm(1,beta3[,t-1],diag(c(1,0.01,1,1,1,0.01,0.01,0.1,1)))) # implementation assumes a random walk. 
  # discard this input for a fixed proposal distribution
  
  # We work with the log-likelihoods for numeric stability.
  logR = logpost(beta.prop)-logpost(beta3[,t-1])
  
  R = exp(logR)
  
  u <- runif(1)        ## uniform variable to determine acceptance
  if(u < R){           ## accept the new value
    beta3[,t] = beta.prop
    accepted3 = c(accepted3,1)
  }    
  else{
    beta3[,t] = beta3[,t-1]     ## reject the new value
    accepted3 = c(accepted3,0)
  }    
}
t3 <- proc.time() - ptm
mean(accepted1) 
```


Neural Network
```{r}
library(neuralnet)
#Scale data for neural network
max = apply(data , 2 , max)
min = apply(data, 2 , min)
scale(data, center = min, scale = max - min)
scaled = as.data.frame(scale(data, center = min, scale = max - min))
## Fit neural network 
set.seed(2)
NN = neuralnet(y ~ , data, hidden = 3 , linear.output = T )
# plot neural network
plot(NN)
```
